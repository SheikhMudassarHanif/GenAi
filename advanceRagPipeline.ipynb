{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SheikhMudassarHanif/GenAi/blob/main/advanceRagPipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e2ea6ab",
      "metadata": {
        "id": "4e2ea6ab"
      },
      "source": [
        "#Setting the APIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c757c61",
      "metadata": {
        "id": "1c757c61"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "os.environ['LANGCHAIN_API_KEY']=os.getenv('langchain_api')\n",
        "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"openai_api_key\")\n",
        "os.environ[\"OPENAI_API_BASE\"]=os.getenv('openai_api_base')\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"]=os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
        "LANGSMITH_TRACING=True\n",
        "os.environ[\"USER_AGENT\"] = \"LangChainFastAPIApp/1.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c68ebf24",
      "metadata": {
        "id": "c68ebf24"
      },
      "source": [
        "Loading the data\n",
        "Transforming the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "807fc99b",
      "metadata": {
        "id": "807fc99b",
        "outputId": "032277d0-5043-4e1d-a646-836456571a39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_23000\\625753536.py:23: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
            "  db=FAISS.from_documents(documents[:30],OllamaEmbeddings())\n"
          ]
        }
      ],
      "source": [
        "#reading from pd\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader=PyPDFLoader('attentionPdf.pdf')\n",
        "docs=loader.load()\n",
        "#Transformation (text spillter from langchain)\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
        "documents=text_splitter.split_documents(docs)\n",
        "# documents[:5]\n",
        "##Creating Embedding using hugging face\n",
        "\n",
        "# from langchain_huggingface import HuggingFaceEmbeddings\n",
        "# embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "# db=FAISS.from_documents(documents[:30],embedding_model)\n",
        "\n",
        "##Creating Embedding using ollama\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "db=FAISS.from_documents(documents[:30],OllamaEmbeddings())\n",
        "#setting up vector databse\n",
        "\n",
        "#Now conversion into vectors Vector embedding and vector store\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af3c7e83",
      "metadata": {
        "id": "af3c7e83",
        "outputId": "7991ba8c-6f7d-44f6-d94f-62a035124e06"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x17775de50d0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9223d890",
      "metadata": {
        "id": "9223d890",
        "outputId": "eb009283-0aba-43aa-f613-573d4bd1271f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the Ô¨Årst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query=\"What is attention is all you need\"\n",
        "result=db.similarity_search(query)\n",
        "result[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e602c6",
      "metadata": {
        "id": "b5e602c6",
        "outputId": "0bdfbe3b-6104-4e16-a42b-277780cbfb05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_23000\\2415645411.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
            "  llm=Ollama(model=\"llama2\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Ollama()"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.llms import Ollama\n",
        "llm=Ollama(model=\"llama2\")\n",
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "418bd0c0",
      "metadata": {
        "id": "418bd0c0"
      },
      "outputs": [],
      "source": [
        "#Design ChatPrompt Template\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt=ChatPromptTemplate.from_template(\n",
        "    \"\"\" Answer the following question based only on the provided context.\n",
        "    Think step by step before providing a detailed answer. I will tip you$1000 if the user finds the answer helpful\n",
        "    <context> {context} </context>\n",
        "    Question: {input}\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "626a5488",
      "metadata": {
        "id": "626a5488"
      },
      "outputs": [],
      "source": [
        "##Chain Introduction\n",
        "\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "document_chain=create_stuff_documents_chain(llm,prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "788b87ba",
      "metadata": {
        "id": "788b87ba",
        "outputId": "f51cf3b1-4ebb-41ee-8b15-aac8e4294ee4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000017775DE50D0>, search_kwargs={})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever=db.as_retriever()\n",
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f451655",
      "metadata": {
        "id": "1f451655"
      },
      "outputs": [],
      "source": [
        "##MY retriever chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f91ec228",
      "metadata": {
        "id": "f91ec228"
      },
      "outputs": [],
      "source": [
        "response=retrieval_chain.invoke({\"input\":\"Scaled Dot-Product Attention\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a06591",
      "metadata": {
        "id": "96a06591",
        "outputId": "d1e878c3-0eaf-47c7-9d22-357da129ee1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The question is about \"Scaled Dot-Product Attention\" in the context of the Transformer model.\\n\\nTo answer this question, we need to understand the role of attention mechanisms in the Transformer architecture. The Transformer model uses self-attention layers in both the encoder and decoder, which allows each position in the sequence to attend to all positions in the sequence. This is different from traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs), where the attention is limited to a specific window of positions.\\n\\nScaled dot-product attention is a type of self-attention mechanism used in the Transformer model. It calculates the attention weight between two positions by taking the dot product of their representations and scaling the result by a scalar value. The scalar value, called the attention scale, controls the amount of attention that is given to each position.\\n\\nThe question asks about preventing leftward information flow in the decoder to preserve the auto-regressive property. This means that the model should not attend to positions further ahead in the sequence than necessary, as this could result in information from later positions leaking back into earlier positions and causing incorrect predictions.\\n\\nTo address this issue, the Transformer model uses a masking technique to prevent illegal connections between positions in the decoder. This is done by setting the attention scale to infinity for any position that would otherwise attend to a subsequent position. This effectively disables the attention connection between these positions, preventing leftward information flow.\\n\\nIn summary, the Transformer model uses scaled dot-product attention to allow each position in the sequence to attend to all positions in the sequence. However, to prevent leftward information flow and preserve the auto-regressive property, the model employs a masking technique to disable illegal connections between positions in the decoder.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1026257c",
      "metadata": {
        "id": "1026257c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "LangChainSeries",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}