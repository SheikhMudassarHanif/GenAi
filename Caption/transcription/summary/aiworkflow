{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:35:16.012104Z","iopub.execute_input":"2025-02-15T05:35:16.012582Z","iopub.status.idle":"2025-02-15T05:35:19.849431Z","shell.execute_reply.started":"2025-02-15T05:35:16.012544Z","shell.execute_reply":"2025-02-15T05:35:19.848327Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.16.0)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\nRequirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.8)\nRequirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.0)\nRequirement already satisfied: gradio-client==1.7.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.7.0)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\nRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.11.0a1)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.20)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\nRequirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.6)\nRequirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\nRequirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\nRequirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.45.3)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\nRequirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (2024.9.0)\nRequirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (14.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.28.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.28.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# import gradio as gr\n# import cv2\n# import torch\n# from transformers import BlipProcessor, BlipForConditionalGeneration\n# import moviepy.editor as mp\n# import os\n\n# def extract_keyframes(video_path, frame_interval=30):\n#     \"\"\"Extracts keyframes from the video at given interval.\"\"\"\n#     cap = cv2.VideoCapture(video_path)\n#     frames = []\n#     count = 0\n#     while cap.isOpened():\n#         ret, frame = cap.read()\n#         if not ret:\n#             break\n#         if count % frame_interval == 0:\n#             frames.append(frame)\n#         count += 1\n#     cap.release()\n#     return frames\n\n# # Load lightweight model from Hugging Face\n# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n# def generate_caption(frame):\n#     \"\"\"Generate caption for a given frame.\"\"\"\n#     frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n#     inputs = processor(frame_rgb, return_tensors=\"pt\")\n#     caption_ids = model.generate(**inputs)\n#     caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n#     return caption\n\n# def add_captions_to_video(video_path):\n#     \"\"\"Process the video by adding captions.\"\"\"\n#     frames = extract_keyframes(video_path)\n#     captions = [generate_caption(frame) for frame in frames]\n    \n#     # Load the original video\n#     video = mp.VideoFileClip(video_path)\n    \n#     # Add captions using moviepy\n#     final_clips = []\n#     for i, caption in enumerate(captions):\n#         # txt_clip = mp.TextClip(caption, fontsize=24, color='white', bg_color='black')\n#         txt_clip = mp.TextClip(caption, fontsize=24, color='white', bg_color='black', method='caption')\n#         txt_clip = txt_clip.set_position(('center', 'bottom')).set_duration(3)\n#         final_clips.append(txt_clip)\n    \n#     final_video = mp.CompositeVideoClip([video] + final_clips)\n#     output_path = \"output_video.mp4\"\n#     final_video.write_videofile(output_path, codec='libx264', fps=video.fps)\n#     return output_path\n\n# def gradio_interface(video):\n#     \"\"\"Gradio interface function.\"\"\"\n#     output_video = add_captions_to_video(video)\n#     return output_video\n\n# iface = gr.Interface(\n#     fn=gradio_interface,\n#     inputs=gr.Video(label=\"Upload Video\"),\n#     outputs=gr.Video(label=\"Captioned Video\"),\n#     title=\"Automatic Video Captioning\",\n#     description=\"Upload a video without captions, and the system will generate and overlay captions automatically.\"\n# )\n\n# if __name__ == \"__main__\":\n#     iface.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:00:11.886220Z","iopub.execute_input":"2025-02-15T05:00:11.886623Z","iopub.status.idle":"2025-02-15T05:00:11.891262Z","shell.execute_reply.started":"2025-02-15T05:00:11.886588Z","shell.execute_reply":"2025-02-15T05:00:11.890258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nimport cv2\nimport torch\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nimport moviepy.editor as mp\nimport numpy as np\n\ndef extract_keyframes(video_path, frame_interval=30):\n    \"\"\"Extracts keyframes from the video at given interval.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if count % frame_interval == 0:\n            frames.append(frame)\n        count += 1\n    cap.release()\n    return frames\n\n# Load lightweight model from Hugging Face\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\ndef generate_caption(frame):\n    \"\"\"Generate caption for a given frame.\"\"\"\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    inputs = processor(frame_rgb, return_tensors=\"pt\")\n    caption_ids = model.generate(**inputs)\n    caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n    return caption\n\ndef overlay_text_on_frame(frame, text, position=(50, 50), font_scale=1, font_thickness=2):\n    \"\"\"Overlays text onto a video frame using OpenCV.\"\"\"\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    text_color = (255, 255, 255)  # White text\n    bg_color = (0, 0, 0)  # Black background\n    (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, font_thickness)\n    \n    # Draw background rectangle\n    x, y = position\n    cv2.rectangle(frame, (x, y - text_height - 5), (x + text_width, y + baseline), bg_color, -1)\n    \n    # Put text on top\n    cv2.putText(frame, text, (x, y), font, font_scale, text_color, font_thickness, cv2.LINE_AA)\n    return frame\n\ndef add_captions_to_video(video_path):\n    \"\"\"Extracts frames, adds captions using OpenCV, and reconstructs video.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    \n    # Define output video writer\n    output_path = \"output_video.mp4\"\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec for MP4\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        if count % 30 == 0:  # Process every 30th frame\n            caption = generate_caption(frame)  # Get caption using the model\n            frame = overlay_text_on_frame(frame, caption)\n        \n        out.write(frame)\n        count += 1\n    \n    cap.release()\n    out.release()\n    \n    return output_path\n\ndef gradio_interface(video):\n    \"\"\"Gradio interface function.\"\"\"\n    output_video = add_captions_to_video(video)\n    return output_video\n\niface = gr.Interface(\n    fn=gradio_interface,\n    inputs=gr.Video(label=\"Upload Video\"),\n    outputs=gr.Video(label=\"Captioned Video\"),\n    title=\"Automatic Video Captioning\",\n    description=\"Upload a video without captions, and the system will generate and overlay captions automatically.\"\n)\n\nif __name__ == \"__main__\":\n    iface.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:00:15.747758Z","iopub.execute_input":"2025-02-15T05:00:15.748118Z","iopub.status.idle":"2025-02-15T05:00:18.867598Z","shell.execute_reply.started":"2025-02-15T05:00:15.748081Z","shell.execute_reply":"2025-02-15T05:00:18.866825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Better Output. Preserving the Audio ","metadata":{}},{"cell_type":"code","source":"import gradio as gr\nimport cv2\nimport torch\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nimport moviepy.editor as mp\nimport numpy as np\n\ndef extract_keyframes(video_path, frame_interval=30):\n    \"\"\"Extracts keyframes from the video at given interval.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if count % frame_interval == 0:\n            frames.append(frame)\n        count += 1\n    cap.release()\n    return frames\n\n# Load lightweight model from Hugging Face\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\ndef generate_caption(frame):\n    \"\"\"Generate caption for a given frame.\"\"\"\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    inputs = processor(frame_rgb, return_tensors=\"pt\")\n    caption_ids = model.generate(**inputs)\n    caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n    return caption\n\ndef overlay_text_on_frame(frame, text, position=(50, 50), font_scale=1, font_thickness=2):\n    \"\"\"Overlays text onto a video frame using OpenCV.\"\"\"\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    text_color = (255, 255, 255)  # White text\n    bg_color = (0, 0, 0)  # Black background\n    (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, font_thickness)\n    \n    # Draw background rectangle\n    x, y = position\n    cv2.rectangle(frame, (x, y - text_height - 5), (x + text_width, y + baseline), bg_color, -1)\n    \n    # Put text on top\n    cv2.putText(frame, text, (x, y), font, font_scale, text_color, font_thickness, cv2.LINE_AA)\n    return frame\n\ndef add_captions_to_video(video_path):\n    \"\"\"Extracts frames, adds captions using OpenCV, and reconstructs video while preserving audio.\"\"\"\n    video = mp.VideoFileClip(video_path)\n    audio = video.audio  # Extract audio\n    cap = cv2.VideoCapture(video_path)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    \n    frames = []\n    count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        if count % 30 == 0:  # Process every 30th frame\n            caption = generate_caption(frame)  # Get caption using the model\n            frame = overlay_text_on_frame(frame, caption)\n        \n        frames.append(frame)\n        count += 1\n    \n    cap.release()\n    \n    # Convert frames to a video clip\n    output_video = mp.ImageSequenceClip(frames, fps=fps)\n    output_video = output_video.set_audio(audio)  # Restore audio\n    output_path = \"output_video.mp4\"\n    output_video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n    \n    return output_path\n\ndef gradio_interface(video):\n    \"\"\"Gradio interface function.\"\"\"\n    output_video = add_captions_to_video(video)\n    return output_video\n\niface = gr.Interface(\n    fn=gradio_interface,\n    inputs=gr.Video(label=\"Upload Video\"),\n    outputs=gr.Video(label=\"Captioned Video\"),\n    title=\"Automatic Video Captioning\",\n    description=\"Upload a video without captions, and the system will generate and overlay captions automatically while preserving audio.\"\n)\n\nif __name__ == \"__main__\":\n    iface.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:03:46.064611Z","iopub.execute_input":"2025-02-15T05:03:46.064981Z","iopub.status.idle":"2025-02-15T05:03:48.051343Z","shell.execute_reply.started":"2025-02-15T05:03:46.064952Z","shell.execute_reply":"2025-02-15T05:03:48.050132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Fixing the captions display","metadata":{}},{"cell_type":"code","source":"import gradio as gr\nimport cv2\nimport torch\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nimport moviepy.editor as mp\nimport numpy as np\n\ndef extract_keyframes(video_path, frame_interval=90):  # Increased interval to reduce frequent changes\n    \"\"\"Extracts keyframes from the video at given interval.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    frame_timestamps = []\n    count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if count % frame_interval == 0:\n            frames.append(frame)\n            frame_timestamps.append(count)\n        count += 1\n    cap.release()\n    return frames, frame_timestamps\n\n# Load lightweight model from Hugging Face\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\ndef generate_caption(frame):\n    \"\"\"Generate caption for a given frame.\"\"\"\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    inputs = processor(frame_rgb, return_tensors=\"pt\")\n    caption_ids = model.generate(**inputs)\n    caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n    return caption\n\ndef overlay_text_on_frame(frame, text, position=(50, 50), font_scale=1, font_thickness=2):\n    \"\"\"Overlays text onto a video frame using OpenCV.\"\"\"\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    text_color = (255, 255, 255)  # White text\n    bg_color = (0, 0, 0)  # Black background\n    (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, font_thickness)\n    \n    # Draw background rectangle\n    x, y = position\n    cv2.rectangle(frame, (x, y - text_height - 5), (x + text_width, y + baseline), bg_color, -1)\n    \n    # Put text on top\n    cv2.putText(frame, text, (x, y), font, font_scale, text_color, font_thickness, cv2.LINE_AA)\n    return frame\n\ndef add_captions_to_video(video_path):\n    \"\"\"Extracts frames, adds captions using OpenCV, and reconstructs video while preserving audio.\"\"\"\n    video = mp.VideoFileClip(video_path)\n    audio = video.audio  # Extract audio\n    cap = cv2.VideoCapture(video_path)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    \n    frames, frame_timestamps = extract_keyframes(video_path)\n    captions = [generate_caption(frame) for frame in frames]\n    \n    full_frames = []\n    count = 0\n    caption_idx = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        if caption_idx < len(frame_timestamps) and count >= frame_timestamps[caption_idx]:\n            frame = overlay_text_on_frame(frame, captions[caption_idx])\n            caption_idx += 1  # Move to the next caption only when required\n        \n        full_frames.append(frame)\n        count += 1\n    \n    cap.release()\n    \n    # Convert frames to a video clip\n    output_video = mp.ImageSequenceClip(full_frames, fps=fps)\n    output_video = output_video.set_audio(audio)  # Restore audio\n    output_path = \"output_video.mp4\"\n    output_video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n    \n    return output_path\n\ndef gradio_interface(video):\n    \"\"\"Gradio interface function.\"\"\"\n    output_video = add_captions_to_video(video)\n    return output_video\n\niface = gr.Interface(\n    fn=gradio_interface,\n    inputs=gr.Video(label=\"Upload Video\"),\n    outputs=gr.Video(label=\"Captioned Video\"),\n    title=\"Automatic Video Captioning\",\n    description=\"Upload a video without captions, and the system will generate and overlay captions automatically while preserving audio.\"\n)\n\nif __name__ == \"__main__\":\n    iface.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:07:05.016572Z","iopub.execute_input":"2025-02-15T05:07:05.017010Z","iopub.status.idle":"2025-02-15T05:07:07.177461Z","shell.execute_reply.started":"2025-02-15T05:07:05.016971Z","shell.execute_reply":"2025-02-15T05:07:07.176660Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"New Feature","metadata":{}},{"cell_type":"code","source":"!pip install SpeechRecognition\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:35:08.358370Z","iopub.execute_input":"2025-02-15T05:35:08.358673Z","iopub.status.idle":"2025-02-15T05:35:12.349221Z","shell.execute_reply.started":"2025-02-15T05:35:08.358643Z","shell.execute_reply":"2025-02-15T05:35:12.348234Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.14.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import gradio as gr\nimport cv2\nimport torch\nfrom transformers import BlipProcessor, BlipForConditionalGeneration, pipeline\nimport moviepy.editor as mp\nimport numpy as np\nimport speech_recognition as sr\n\n# Initialize HuggingFace models for captioning, summarization, and transcription\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nsummarizer = pipeline(\"summarization\")\nspeech_recognizer = sr.Recognizer()\n\ndef extract_keyframes(video_path, frame_interval=30):\n    \"\"\"Extracts keyframes from the video at given interval.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if count % frame_interval == 0:\n            frames.append(frame)\n        count += 1\n    cap.release()\n    return frames\n\ndef generate_caption(frame):\n    \"\"\"Generate caption for a given frame.\"\"\"\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    inputs = processor(frame_rgb, return_tensors=\"pt\")\n    caption_ids = model.generate(**inputs)\n    caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n    return caption\n\ndef overlay_text_on_frame(frame, text, position=(50, 50), font_scale=1, font_thickness=2):\n    \"\"\"Overlays text onto a video frame using OpenCV.\"\"\"\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    text_color = (255, 255, 255)  # White text\n    bg_color = (0, 0, 0)  # Black background\n    (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, font_thickness)\n    \n    # Draw background rectangle\n    x, y = position\n    cv2.rectangle(frame, (x, y - text_height - 5), (x + text_width, y + baseline), bg_color, -1)\n    \n    # Put text on top\n    cv2.putText(frame, text, (x, y), font, font_scale, text_color, font_thickness, cv2.LINE_AA)\n    return frame\n\ndef transcribe_audio_from_video(video_path):\n    \"\"\"Transcribe audio from a video.\"\"\"\n    video = mp.VideoFileClip(video_path)\n    audio = video.audio\n    audio_path = \"temp_audio.wav\"\n    audio.write_audiofile(audio_path)\n    \n    with sr.AudioFile(audio_path) as source:\n        audio_data = speech_recognizer.record(source)\n        transcription = speech_recognizer.recognize_google(audio_data)\n    \n    return transcription\n\n# def summarize_text(text):\n#     \"\"\"Generate a summary of the transcribed text.\"\"\"\n#     summary = summarizer(text, max_length=150, min_length=50, do_sample=False)\n#     return summary[0]['summary_text']\n\ndef summarize_text(text):\n    \"\"\"Generate a summary of the transcribed text.\"\"\"\n    # Adjust max_length based on the length of the input\n    input_length = len(text.split())\n    max_length = min(input_length, 50)  # You can tweak 50 to fit your needs\n    summary = summarizer(text, max_length=max_length, min_length=10, do_sample=False)\n    return summary[0]['summary_text']\n\n\ndef add_captions_to_video(video_path):\n    \"\"\"Extracts frames, adds captions using OpenCV, and reconstructs video while preserving audio.\"\"\"\n    video = mp.VideoFileClip(video_path)\n    audio = video.audio  # Extract audio\n    cap = cv2.VideoCapture(video_path)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    \n    frames = []\n    count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        if count % 30 == 0:  # Process every 30th frame\n            caption = generate_caption(frame)  # Get caption using the model\n            frame = overlay_text_on_frame(frame, caption)\n        \n        frames.append(frame)\n        count += 1\n    \n    cap.release()\n    \n    # Convert frames to a video clip\n    output_video = mp.ImageSequenceClip(frames, fps=fps)\n    output_video = output_video.set_audio(audio)  # Restore audio\n    output_path = \"output_video.mp4\"\n    output_video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n    \n    return output_path\n\ndef gradio_interface(video):\n    \"\"\"Gradio interface function.\"\"\"\n    transcription = transcribe_audio_from_video(video)  # Get transcription\n    summary = summarize_text(transcription)  # Get summary of transcription\n    output_video = add_captions_to_video(video)  # Add captions to video\n    \n    return output_video, transcription, summary\n\niface = gr.Interface(\n    fn=gradio_interface,\n    inputs=gr.Video(label=\"Upload Video\"),\n    outputs=[gr.Video(label=\"Captioned Video\"), gr.Textbox(label=\"Transcription\"), gr.Textbox(label=\"Summary\")],\n    title=\"Automatic Video Captioning and Transcription\",\n    description=\"Upload a video, and the system will generate captions, transcribe the audio, and provide a summary of the transcription.\"\n)\n\nif __name__ == \"__main__\":\n    iface.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:35:42.715189Z","iopub.execute_input":"2025-02-15T05:35:42.715608Z","iopub.status.idle":"2025-02-15T05:35:47.327210Z","shell.execute_reply.started":"2025-02-15T05:35:42.715575Z","shell.execute_reply":"2025-02-15T05:35:47.326050Z"}},"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7868\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://0f273284d520b4b1fd.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://0f273284d520b4b1fd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"MoviePy - Writing audio in temp_audio.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    ","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Moviepy - Building video output_video.mp4.\nMoviePy - Writing audio in output_videoTEMP_MPY_wvf_snd.mp4\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nMoviepy - Writing video output_video.mp4\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Moviepy - Done !\nMoviepy - video ready output_video.mp4\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}